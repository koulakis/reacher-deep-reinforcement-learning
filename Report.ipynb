{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Continuous Control\n",
    "\n",
    "This project was a trigger to explore reinforcement learning frameworks. The framework which was used to solve the problem is [stable baselines 3](https://stable-baselines3.readthedocs.io/en/master/) which is implemented with pytorch. The problem was solved using A2C, PPO and TD3 on the 20 agent environment. Below one can find a description of each algorithm with references to the codebase, notes and graphs from the experiments, and finally a comparison of the effectiveness of the three algorithms on the problem at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few words about stable baselines 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the library\n",
    "The library has an interesting story. In an attempt to standardise implementations and give the ability to use recent reinforcement learning algorithms out of the box to benchmark problems, OpenAI created the library baselines. Many complained about its sketchy implementation and lack of documentation. This led some users to fork the library and create a friendlier version with cleaner code, stable baselines. The main library is based on TensorFlow, but recently they introduced a new version of it, stable baselines 3, which uses PyTorch. The library has currently A2C, PPO and TD3 implemented and gives the ability to the user to set different hyper parameters and define the architectures of the policy and value networks. \n",
    "\n",
    "It must be stated that there are several other libraries which have similar structure. Ones which were considered were reagent, dopamine and kerasrl. Aside from those, one can find several implementations of more recent or specialized algorithms via paperswithcode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping the Unity environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging on tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for the future\n",
    "\n",
    "### Learning faster\n",
    "The solution to the problem is extremely close to the optimal score, 40, so there is not a lot to improve regarding performance. On the other hand one could explore options to make the convergence of training faster. Here are some ideas in this direction:\n",
    "- Tune the size of the rollout and the number of epochs: Up to now the default values of 2048 and 10 epochs\n",
    "- Mixed precision training: This a generic low-level improvement. In a lot of use cases, especially in computer vision, one can sucessfully reduce the float precision to 16 bit on specific parts of the networks trained and achiece speed increase in both training and predicting with minimal drops in performance. Curious if this works equally well in the reinforcement learning setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
