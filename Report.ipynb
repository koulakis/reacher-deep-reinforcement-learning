{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Continuous Control\n",
    "\n",
    "This project was a trigger to explore reinforcement learning frameworks. The framework which was used to solve the problem is [stable baselines 3](https://stable-baselines3.readthedocs.io/en/master/) which is implemented with pytorch. The problem was solved using A2C, PPO and TD3 on the 20 agent environment. Below one can find a description of each algorithm with references to the codebase, notes and graphs from the experiments, and finally a comparison of the effectiveness of the three algorithms on the problem at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of stable baselines 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the library\n",
    "The library has an interesting story. In an attempt to standardise implementations and give the ability to use recent reinforcement learning algorithms out of the box to benchmark problems, OpenAI created the library [baselines](https://github.com/openai/baselines). Though the project was sucessful, it still lacked consistency and ease of use. This led some users to fork the library and create a friendlier version with cleaner code, [stable baselines](https://stable-baselines.readthedocs.io/en/master). The main library is based on TensorFlow, but recently a new version of it was introduced, [stable baselines 3](https://stable-baselines3.readthedocs.io/en/master/), which uses PyTorch. The library has currently A2C, PPO and TD3 implemented and gives the ability to the user to set different hyper parameters and define the architectures of the policy and value networks. \n",
    "\n",
    "It must be stated that there are several other libraries which have similar structure. Ones which were considered were [reagent](https://github.com/facebookresearch/ReAgent), [dopamine](https://github.com/google/dopamine) and [kerasrl](https://keras-rl.readthedocs.io/en/latest/). Aside from those, one can find several implementations of more recent or specialized algorithms via [paperswithcode](https://paperswithcode.com/area/playing-games). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping the Unity environments\n",
    "One of the steps needed in order to use the library was to wrap the Unity environments in an environment class compatible with it. One can find the wrappers' code in `reacher/unity_environment_wrappers.py`. Two wrappers were built:\n",
    "- One for the single agent environment. This was straight forward as one only needed to translate the Unity environment to a OpenAI gym environment.\n",
    "- One for the multi-agent environment. Unfortunately stable baselines support only multiple environments with single agents in this direction. Nevertheless, the one can define a customized environment inheriting from `VecEnv` and handle the multiple agent actions as if they come from different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning algorithms\n",
    "Below is a description of A2C, PPO and TD3. I attempt below to connect the algorithms with their implementation in the codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout and prioritized replay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and solution of the enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO trained on the single agent environment\n",
    "- Setting the network architecture\n",
    "- Picking the right learning rate\n",
    "- Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO trained on the multi-agent environment\n",
    "- Big speed-up\n",
    "- More robust on learning rate changes\n",
    "- Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C\n",
    "- Trained also on  the multi-agent environment\n",
    "- Sometimes converges really fast\n",
    "- Not as consistent in the results. Some times converges, some not.\n",
    "- Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD3\n",
    "-\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for the future\n",
    "\n",
    "### Learning faster\n",
    "The solution to the problem is extremely close to the optimal score, 40, so there is not a lot to improve regarding performance. On the other hand one could explore options to make the convergence of training faster. Here are some ideas in this direction:\n",
    "- *Tune the size of the rollout and the number of epochs.* Up to now the default values of 2048 environment steps till rollout and 10 epochs for training. One could do an ablation study on both parameters and check how the reduction of the 'collect data -> train' cycle affects the speed of convergence.\n",
    "- Mixed precision training: This a generic low-level improvement. In a lot of use cases, especially in computer vision, one can sucessfully reduce the float precision to 16 bit on specific parts of the networks trained and achiece speed increase in both training and predicting with minimal drops in performance. Curious if this works equally well in the reinforcement learning setting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
